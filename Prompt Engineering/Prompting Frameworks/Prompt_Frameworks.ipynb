{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8pBcsNP33SjLYbVd/jkXc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreevidhya08/Generative-AI/blob/main/Prompt%20Engineering/Prompting%20Frameworks/Prompt_Frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba4LsgvXstwc",
        "outputId": "13b92a5c-e6c1-4482-8961-d6e0ce7d761a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(api_key=\"insert your groq api key \")"
      ],
      "metadata": {
        "id": "-koZHghosxgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to call LLMs"
      ],
      "metadata": {
        "id": "9mN5pbr1tIwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(messages, temperature=0.3):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "Gxl-rRGRs2iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERSONA PROMPT"
      ],
      "metadata": {
        "id": "FmSHYKfPtbS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PERSONA_PROMPT = \"\"\"\n",
        "You are an AI and ML expert with 10+ years of industry experience.\n",
        "You mentor fresh graduates and interns. You consistently behave as a mentor,\n",
        "explaining concepts from fundamentals to real-world applications in simple language.\n",
        "Do not hallucinate facts and do not give excess data.Prefer accuracy over creativity\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": PERSONA_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Explain Text preprocessing\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVxUr3xDtlZZ",
        "outputId": "77c68014-8599-4b74-e18a-040828ec5987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing is a crucial step in natural language processing (NLP) and machine learning (ML) pipelines. It involves cleaning and normalizing text data to prepare it for analysis or modeling. Here's a simplified overview:\n",
            "\n",
            "**Why is text preprocessing necessary?**\n",
            "\n",
            "Text data can be noisy, inconsistent, and unstructured, which can negatively impact the performance of NLP and ML models. Text preprocessing helps to:\n",
            "\n",
            "1. Remove irrelevant information\n",
            "2. Reduce noise and inconsistencies\n",
            "3. Normalize the data format\n",
            "4. Improve model accuracy and efficiency\n",
            "\n",
            "**Common text preprocessing techniques:**\n",
            "\n",
            "1. **Tokenization**: breaking down text into individual words or tokens.\n",
            "2. **Stopword removal**: removing common words like \"the\", \"and\", \"a\", etc. that don't add much value to the meaning.\n",
            "3. **Stemming or Lemmatization**: reducing words to their base form (e.g., \"running\" becomes \"run\").\n",
            "4. **Removing special characters and punctuation**: eliminating characters that don't contribute to the meaning.\n",
            "5. **Removing numbers and dates**: unless they're relevant to the analysis.\n",
            "6. **Converting to lowercase**: ensuring all text is in a consistent case.\n",
            "7. **Removing HTML tags and URLs**: if present in the text data.\n",
            "8. **Handling missing values**: deciding what to do with empty or missing text fields.\n",
            "\n",
            "**Real-world applications:**\n",
            "\n",
            "Text preprocessing is essential in various applications, such as:\n",
            "\n",
            "1. **Sentiment analysis**: analyzing customer reviews or feedback.\n",
            "2. **Text classification**: categorizing text into predefined categories (e.g., spam vs. non-spam emails).\n",
            "3. **Information retrieval**: searching for relevant documents or articles.\n",
            "4. **Chatbots and virtual assistants**: understanding and responding to user queries.\n",
            "\n",
            "Remember, the specific preprocessing techniques used may vary depending on the project requirements and the nature of the text data. As a mentor, I always emphasize the importance of understanding the data and the problem you're trying to solve before applying any preprocessing techniques.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROOT PROMPT"
      ],
      "metadata": {
        "id": "SIjNGMgo3LRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PROMPT = \"\"\"\n",
        "You are an AI mentor with 10+ years of industry experience.\n",
        "Explain concepts step-by-step.\n",
        "Prefer accuracy over creativity.\n",
        "Avoid hallucinations.\n",
        "\"\"\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": ROOT_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Explain Prompting\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScRiDHlT25sD",
        "outputId": "7d4aa56e-398b-4aa1-8e1f-54130246bec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompting is a crucial concept in natural language processing (NLP) and artificial intelligence (AI). I'll break it down step by step.\n",
            "\n",
            "**What is Prompting?**\n",
            "\n",
            "Prompting refers to the process of providing a text-based input or cue to an AI model, such as a language model or chatbot, to elicit a specific response or generate text. The prompt serves as a starting point or context for the AI model to understand what is being asked or what task needs to be performed.\n",
            "\n",
            "**Types of Prompts**\n",
            "\n",
            "There are several types of prompts, including:\n",
            "\n",
            "1. **Open-ended prompts**: These prompts allow the AI model to generate text freely, without any specific constraints or guidelines. Example: \"Write a story about a character who discovers a hidden world.\"\n",
            "2. **Specific prompts**: These prompts provide clear guidelines or constraints for the AI model to follow. Example: \"Write a 5-paragraph essay on the benefits of recycling, using at least 3 examples.\"\n",
            "3. **Conversational prompts**: These prompts simulate a conversation, where the AI model responds to a question or statement. Example: \"What is the weather like today?\"\n",
            "4. **Task-oriented prompts**: These prompts require the AI model to perform a specific task, such as summarizing a text or translating a sentence.\n",
            "\n",
            "**How Prompting Works**\n",
            "\n",
            "When a prompt is provided to an AI model, the following steps occur:\n",
            "\n",
            "1. **Text Analysis**: The AI model analyzes the prompt to understand its meaning, context, and intent.\n",
            "2. **Knowledge Retrieval**: The AI model retrieves relevant knowledge and information from its training data or knowledge base.\n",
            "3. **Generation**: The AI model generates text based on the prompt, using the retrieved knowledge and information.\n",
            "4. **Post-processing**: The generated text may undergo post-processing, such as spell-checking, grammar-checking, or fluency evaluation.\n",
            "\n",
            "**Best Practices for Prompting**\n",
            "\n",
            "To get the most out of prompting, follow these best practices:\n",
            "\n",
            "1. **Clear and concise language**: Use simple and clear language in your prompts to avoid ambiguity.\n",
            "2. **Specificity**: Provide specific guidelines or constraints to help the AI model understand what is expected.\n",
            "3. **Context**: Provide sufficient context for the AI model to understand the topic or task.\n",
            "4. **Evaluation**: Evaluate the generated text to ensure it meets your requirements and make adjustments to the prompt as needed.\n",
            "\n",
            "By understanding how prompting works and following best practices, you can effectively use AI models to generate high-quality text, answer questions, or perform tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COGNITIVE VERIFIER PATTERN"
      ],
      "metadata": {
        "id": "YTsHvU0Mteio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "Explain cognitive verifier pattern in prompt engineering.\n",
        "\n",
        "Now verify your answer:\n",
        "- Is it correct?\n",
        "- Is anything missing?\n",
        "- Improve the explanation if needed.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tvIA3zKtkJ1",
        "outputId": "81a03941-66e4-40ae-f05f-6800aef8eb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cognitive verifier pattern is a technique used in prompt engineering to improve the accuracy and reliability of language models. It involves adding a verification step to the prompt, where the model is asked to evaluate the correctness of its own response or a given statement.\n",
            "\n",
            "The pattern typically consists of two parts:\n",
            "\n",
            "1. **Initial prompt**: The model is given a task or question to answer, such as \"What is the capital of France?\"\n",
            "2. **Verification prompt**: The model is then asked to verify its own response, such as \"Is Paris the capital of France?\" or \"Is the following statement correct: 'The capital of France is Paris'?\"\n",
            "\n",
            "By using this pattern, the model is forced to critically evaluate its own response and provide a more accurate answer. This can help to:\n",
            "\n",
            "* Reduce errors and inconsistencies\n",
            "* Improve the model's ability to recognize and correct its own mistakes\n",
            "* Increase the overall reliability and trustworthiness of the model's responses\n",
            "\n",
            "Verification:\n",
            "\n",
            "* **Is it correct?**: Yes, the explanation is correct. The cognitive verifier pattern is a technique used to improve the accuracy and reliability of language models by adding a verification step to the prompt.\n",
            "* **Is anything missing?**: The explanation could be improved by providing more examples of how the cognitive verifier pattern can be applied in different contexts, such as natural language processing, question answering, or text generation.\n",
            "* **Improve the explanation if needed**: To improve the explanation, more details can be added about the benefits and limitations of the cognitive verifier pattern. For example, the pattern can be useful for tasks that require high accuracy and reliability, but it may not be suitable for tasks that require creativity or open-ended responses. Additionally, the explanation can be enhanced by providing more examples of how the pattern can be used in combination with other prompt engineering techniques to achieve better results.\n",
            "\n",
            "Improved explanation:\n",
            "\n",
            "The cognitive verifier pattern is a powerful technique used in prompt engineering to improve the accuracy and reliability of language models. By adding a verification step to the prompt, the model is forced to critically evaluate its own response and provide a more accurate answer. This pattern can be applied in various contexts, such as natural language processing, question answering, or text generation.\n",
            "\n",
            "For example, in a question answering task, the cognitive verifier pattern can be used as follows:\n",
            "\n",
            "Initial prompt: \"What is the capital of France?\"\n",
            "Verification prompt: \"Is the following statement correct: 'The capital of France is Paris'?\"\n",
            "\n",
            "By using this pattern, the model can provide a more accurate answer and increase the overall reliability and trustworthiness of its responses. The cognitive verifier pattern can be used in combination with other prompt engineering techniques, such as priming or chaining, to achieve better results. However, it may not be suitable for tasks that require creativity or open-ended responses, as it can limit the model's ability to generate novel or innovative answers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION REFINEMENT PATTERN"
      ],
      "metadata": {
        "id": "Z9yDgNjetmdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"User question: \"Explain AI\"\n",
        "\n",
        "Refine this question into:\n",
        "1. Beginner-friendly\n",
        "2. Technical\n",
        "3. Research-level\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8esWPAEtuT2",
        "outputId": "61cfa50a-7920-47e8-ffc4-efe58db7f534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the refined questions:\n",
            "\n",
            "**1. Beginner-friendly:** \"What is Artificial Intelligence (AI) and how does it work in simple terms?\"\n",
            "\n",
            "This question is designed to elicit a basic explanation of AI, its core concepts, and its applications, without delving into technical jargon or complex details.\n",
            "\n",
            "**2. Technical:** \"Can you explain the fundamental principles and architectures of Artificial Intelligence, including machine learning, deep learning, and neural networks?\"\n",
            "\n",
            "This question assumes some background knowledge of computer science and technology, and seeks a more in-depth explanation of AI's technical aspects, including its various subfields, algorithms, and implementation details.\n",
            "\n",
            "**3. Research-level:** \"What are the current advancements and open challenges in Artificial Intelligence research, particularly in areas such as explainability, transparency, and edge AI, and how are they addressing the limitations of traditional machine learning approaches?\"\n",
            "\n",
            "This question is geared towards experts and researchers in the field, and seeks a detailed and nuanced discussion of the latest developments, trends, and research directions in AI, including the most pressing challenges and opportunities for innovation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROVIDING NEW INFORMATION AND ASK QUESTIONS"
      ],
      "metadata": {
        "id": "91B0Ds5Htyvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "Explain CNN briefly.\n",
        "Introduce one new concept related to CNN.\n",
        "Ask 2 follow-up questions.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSotb5Aot3nt",
        "outputId": "2ca81ad2-c5c7-4b32-f3ea-f4f50b4f6829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Brief Introduction to CNN:**\n",
            "A Convolutional Neural Network (CNN) is a type of deep learning model that is commonly used for image and video processing tasks. It consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers, which work together to extract features from input data and make predictions.\n",
            "\n",
            "**New Concept:**\n",
            "One new concept related to CNN is \"Attention Mechanism\". Attention mechanism is a technique used in CNNs to focus on specific parts of the input data that are relevant to the task at hand. This is particularly useful in tasks such as image captioning, where the model needs to focus on specific objects or regions in the image to generate a caption.\n",
            "\n",
            "**Follow-up Questions:**\n",
            "1. How can attention mechanisms be used to improve the performance of CNNs in tasks such as object detection and image segmentation?\n",
            "2. Can attention mechanisms be used in combination with other techniques, such as transfer learning, to further improve the performance of CNNs in computer vision tasks?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHAIN OF THOUGHT"
      ],
      "metadata": {
        "id": "ArnVVwTNxU3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "Explain TF-IDF in steps:\n",
        "1. Explain Term Frequency\n",
        "2. Explain Inverse Document Frequency\n",
        "3. Combine both\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoTh7_vqxXWl",
        "outputId": "570f9914-7bef-4161-a742-3d14ac52b63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's an explanation of TF-IDF in steps:\n",
            "\n",
            "**Step 1: Term Frequency (TF)**\n",
            "Term Frequency is a measure of how often a word appears in a document. It's calculated by dividing the number of times a word appears in a document by the total number of words in the document. The formula for TF is:\n",
            "\n",
            "TF = (Number of times a word appears in a document) / (Total number of words in the document)\n",
            "\n",
            "For example, let's say we have a document with the following text: \"The quick brown fox jumps over the lazy dog.\" If we want to calculate the TF of the word \"the\", we would count the number of times it appears (2) and divide it by the total number of words in the document (9).\n",
            "\n",
            "TF(\"the\") = 2/9 = 0.22\n",
            "\n",
            "This means that the word \"the\" appears approximately 22% of the time in the document.\n",
            "\n",
            "**Step 2: Inverse Document Frequency (IDF)**\n",
            "Inverse Document Frequency is a measure of how rare a word is across a collection of documents. It's calculated by taking the logarithm of the total number of documents divided by the number of documents that contain the word. The formula for IDF is:\n",
            "\n",
            "IDF = log((Total number of documents) / (Number of documents that contain the word))\n",
            "\n",
            "For example, let's say we have a collection of 100 documents, and the word \"the\" appears in 90 of them. We would calculate the IDF of the word \"the\" as follows:\n",
            "\n",
            "IDF(\"the\") = log(100/90) = log(1.11) = 0.05\n",
            "\n",
            "This means that the word \"the\" is very common across the collection of documents, and therefore has a low IDF score.\n",
            "\n",
            "On the other hand, if we have a word that appears in only 1 document out of 100, its IDF score would be much higher:\n",
            "\n",
            "IDF(\"rare_word\") = log(100/1) = log(100) = 2.00\n",
            "\n",
            "This means that the word \"rare_word\" is very rare across the collection of documents, and therefore has a high IDF score.\n",
            "\n",
            "**Step 3: Combining TF and IDF (TF-IDF)**\n",
            "To calculate the TF-IDF score of a word, we multiply its TF score by its IDF score. The formula for TF-IDF is:\n",
            "\n",
            "TF-IDF = TF x IDF\n",
            "\n",
            "Using the examples from above, we can calculate the TF-IDF score of the word \"the\" as follows:\n",
            "\n",
            "TF-IDF(\"the\") = TF(\"the\") x IDF(\"the\") = 0.22 x 0.05 = 0.011\n",
            "\n",
            "And for the word \"rare_word\":\n",
            "\n",
            "TF-IDF(\"rare_word\") = TF(\"rare_word\") x IDF(\"rare_word\") = 0.01 x 2.00 = 0.02\n",
            "\n",
            "The TF-IDF score takes into account both the frequency of a word in a document (TF) and its rarity across a collection of documents (IDF). This allows us to distinguish between common words that appear frequently in a document (but are not very informative) and rare words that appear infrequently in a document (but are very informative). The TF-IDF score can be used in a variety of applications, such as text classification, information retrieval, and topic modeling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TABULAR FORMAT"
      ],
      "metadata": {
        "id": "7qG1BZ_vxZWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "Compare different types of prompting frameworks.\n",
        "Return output in a table, make the columns and rows neatly aligned:\n",
        "Type | Description | Example Algorithm\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvQdYXhLykgN",
        "outputId": "895d619f-1437-454d-fcd1-979732b047d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a comparison of different types of prompting frameworks in a neatly aligned table:\n",
            "\n",
            "| Type                  | Description                                                  | Example Algorithm                                             |\n",
            "| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
            "| Zero-Shot Prompting   | Provides a task description without examples                | \"Write a story about a character who learns a new skill\"    |\n",
            "| Few-Shot Prompting    | Provides a few examples to demonstrate the task              | \"Write a story about a character who learns a new skill. Example: Harry Potter learns to cast spells\" |\n",
            "| Chain-of-Thought      | Encourages the model to generate intermediate steps         | \"Write a step-by-step guide on how to plan a vacation. Show your work\" |\n",
            "| Least-to-Most Prompt | Gradually increases the complexity of the task              | \"First, write a sentence about a cat. Then, write a paragraph about a cat. Finally, write a short story about a cat\" |\n",
            "| Priming               | Uses a sequence of words or phrases to influence the output  | \"The sun was shining, the birds were singing, and the character felt...\" |\n",
            "| Role-Playing Prompt   | Assigns a role to the model to generate a specific response  | \"Act as a customer service representative and respond to this complaint\" |\n",
            "| Recursive Prompting   | Uses the model's own output as input for the next iteration | \"Generate a story, then use the last sentence as the prompt for the next story\" |\n",
            "| Hybrid Prompting      | Combines multiple prompting techniques to achieve a goal   | \"Use few-shot prompting to demonstrate a task, then use chain-of-thought to generate a step-by-step guide\" |\n",
            "\n",
            "Note: These examples are simplified and may not represent the full complexity of each prompting framework.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FILL IN THE BLANKS"
      ],
      "metadata": {
        "id": "FjoEjsfuy5FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "Fill in the blanks:\n",
        "\n",
        "TF-IDF stands for _____.\n",
        "TF measures _____.\n",
        "IDF measures _____.\n",
        "TF_IDF measures _____.\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK4utFvey9_m",
        "outputId": "57c4d461-85df-441d-d1dc-a6ac678bbbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the completed sentences:\n",
            "\n",
            "1. TF-IDF stands for Term Frequency-Inverse Document Frequency.\n",
            "2. TF measures the frequency of a term in a document.\n",
            "3. IDF measures the rarity of a term across a collection of documents.\n",
            "4. TF-IDF measures the importance of a term in a document based on its frequency and rarity across the entire collection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROAL-GOAL-CONTEXT"
      ],
      "metadata": {
        "id": "PuvRZzPSy-RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RGC_PROMPT = \"\"\"\n",
        "Role: You are a data science mentor.\n",
        "Goal: Teach Bigram and Trigram clearly.\n",
        "Context: Student knows Python but is new to NLP.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": RGC_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Explain Bigram and Trigram with example\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZBmqGpxzDO1",
        "outputId": "e8d0c202-f006-427d-c4d7-1683962d8a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Introduction to Bigram and Trigram**\n",
            "=====================================\n",
            "\n",
            "In Natural Language Processing (NLP), Bigram and Trigram are fundamental concepts used to analyze and understand the structure of language. They are used to extract patterns and relationships between words in a sentence or text.\n",
            "\n",
            "**What is a Bigram?**\n",
            "--------------------\n",
            "\n",
            "A Bigram is a sequence of two adjacent words in a sentence or text. It is a way to represent the co-occurrence of two words in a language. Bigrams are used to analyze the probability of a word given the previous word.\n",
            "\n",
            "**Example of Bigram:**\n",
            "----------------------\n",
            "\n",
            " Sentence: \"I love to eat pizza\"\n",
            "\n",
            "Bigrams:\n",
            "\n",
            "* \"I love\"\n",
            "* \"love to\"\n",
            "* \"to eat\"\n",
            "* \"eat pizza\"\n",
            "\n",
            "**What is a Trigram?**\n",
            "---------------------\n",
            "\n",
            "A Trigram is a sequence of three adjacent words in a sentence or text. It is a way to represent the co-occurrence of three words in a language. Trigrams are used to analyze the probability of a word given the previous two words.\n",
            "\n",
            "**Example of Trigram:**\n",
            "----------------------\n",
            "\n",
            " Sentence: \"I love to eat pizza\"\n",
            "\n",
            "Trigrams:\n",
            "\n",
            "* \"I love to\"\n",
            "* \"love to eat\"\n",
            "* \"to eat pizza\"\n",
            "\n",
            "**Python Code to Generate Bigram and Trigram:**\n",
            "----------------------------------------------\n",
            "\n",
            "```python\n",
            "import re\n",
            "from nltk.util import ngrams\n",
            "\n",
            "def generate_bigram_trigram(sentence):\n",
            "    # Tokenize the sentence\n",
            "    tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
            "    \n",
            "    # Generate Bigrams\n",
            "    bigrams = list(ngrams(tokens, 2))\n",
            "    \n",
            "    # Generate Trigrams\n",
            "    trigrams = list(ngrams(tokens, 3))\n",
            "    \n",
            "    return bigrams, trigrams\n",
            "\n",
            "sentence = \"I love to eat pizza\"\n",
            "bigrams, trigrams = generate_bigram_trigram(sentence)\n",
            "\n",
            "print(\"Bigrams:\")\n",
            "for bigram in bigrams:\n",
            "    print(bigram)\n",
            "\n",
            "print(\"\\nTrigrams:\")\n",
            "for trigram in trigrams:\n",
            "    print(trigram)\n",
            "```\n",
            "\n",
            "This code uses the NLTK library to generate Bigrams and Trigrams from a given sentence. The `ngrams` function is used to generate the Bigrams and Trigrams.\n",
            "\n",
            "**Use Cases:**\n",
            "--------------\n",
            "\n",
            "Bigrams and Trigrams have many use cases in NLP, such as:\n",
            "\n",
            "* Language modeling: Bigrams and Trigrams are used to predict the next word in a sentence.\n",
            "* Text classification: Bigrams and Trigrams are used as features to classify text into different categories.\n",
            "* Sentiment analysis: Bigrams and Trigrams are used to analyze the sentiment of text.\n",
            "\n",
            "I hope this explanation helps you understand Bigram and Trigram concepts in NLP. Let me know if you have any further questions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZERO-SHOT PROMPTING"
      ],
      "metadata": {
        "id": "_zjoXs57zDgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Classify sentiment: 'The product is not of expected quality!'\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBtatT10zM2V",
        "outputId": "10e3c9e5-1d7a-4951-80a4-7c3487f30285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the statement \"The product is not of expected quality!\" is NEGATIVE. The use of the word \"not\" and the phrase \"expected quality\" implies that the product has failed to meet the user's expectations, indicating dissatisfaction. The tone is also somewhat emphatic, with the exclamation mark emphasizing the disappointment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONE-SHOT PROMPTING"
      ],
      "metadata": {
        "id": "kVyK0EcPzNN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "Text: \"I hate delays\"\n",
        "Sentiment: Negative\n",
        "\n",
        "Text: \"This movie is fantastic\"\n",
        "Sentiment:\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z8b1moUzQGE",
        "outputId": "61d89b29-4d8c-4fa6-8c72-7ff2f9656cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEW_SHOTS PROMPTING"
      ],
      "metadata": {
        "id": "4idxWIxwzQgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "Text: \"I like this product\"\n",
        "Sentiment: Positive\n",
        "\n",
        "Text: \"Worst experience\"\n",
        "Sentiment: Negative\n",
        "\n",
        "Text: \"It was okay\"\n",
        "Sentiment: Neutral\n",
        "\n",
        "Text: \" Not an amazing service\"\n",
        "Sentiment:\n",
        "\"\"\"}\n",
        "]\n",
        "\n",
        "print(call_llm(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkDgU4MA0V_P",
        "outputId": "3488119d-771d-42fb-a64f-07554f351ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative \n",
            "\n",
            "(The phrase \"Not an amazing service\" implies a somewhat negative sentiment, as it sets a high expectation with the word \"amazing\" and then negates it, suggesting that the service did not meet that expectation.)\n"
          ]
        }
      ]
    }
  ]
}